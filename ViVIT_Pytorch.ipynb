{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djsg2021utec/TESIS_MAESTRIA/blob/main/ViVIT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsHcmerqsv_K"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bNZsGFjbtENG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "from base64 import b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from matplotlib import pyplot as plt\n",
        "import glob\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import shutil\n",
        "import sys\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4resr--RHFW"
      },
      "source": [
        "### Extraer los videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Nw8e2k3RLnV"
      },
      "outputs": [],
      "source": [
        "# Enlace del repositorio donde está alojandos los datos\n",
        "# https://github.com/mchengny/RWF2000-Video-Database-for-Violence-Detection\n",
        "\n",
        "\n",
        "#%pip install gdown (Descomentar si requiere instalar)\n",
        "\n",
        "# ********************************************************************\n",
        "# Descargando los fragmentos del archivo ZIP de la data RWF-2000.zip\n",
        "# ********************************************************************\n",
        "download_links = [\n",
        "    \"https://drive.google.com/uc?id=1nQ9IR3cGc4NEDOXhPQ89id8je8Uj2VUc\",\n",
        "    \"https://drive.google.com/uc?id=1w9G_Z7gkXZzK4DImdI8wanyjs22fQARO\",\n",
        "    \"https://drive.google.com/uc?id=15LhjavoUsLS01CPkc3qav0rJxBc9d4nl\"\n",
        "]\n",
        "\n",
        "for link in tqdm(download_links, desc=\"Descargando fragmentos\"):\n",
        "    !gdown {link}\n",
        "# ********************************************************************\n",
        "# Juntando los fragmentos del archivo ZIP de la data RWF-2000.zip\n",
        "# ********************************************************************\n",
        "!cat RWF-2000.zip.001 RWF-2000.zip.002 RWF-2000.zip.003 > RWF-2000.zip\n",
        "\n",
        "# Eliminando los fragmentos\n",
        "for fragment in tqdm([\"RWF-2000.zip.001\", \"RWF-2000.zip.002\", \"RWF-2000.zip.003\"], desc=\"Eliminando fragmentos\"):\n",
        "    !rm /content/{fragment}\n",
        "\n",
        "# Descomprimiendo los archivos en el directorio RWF-2000\n",
        "!unzip \"/content/RWF-2000.zip\" -d \"/content/\"\n",
        "\n",
        "# Eliminando RWF-2000.zip\n",
        "!rm /content/RWF-2000.zip\n",
        "\n",
        "# Asegurar que la memoria RAM se liberó\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-QkCcT4TXq8a"
      },
      "outputs": [],
      "source": [
        "# ******************************************************************\n",
        "# Se crea un dataframe para manejar fácilmente los datos de RWF-2000\n",
        "# ******************************************************************\n",
        "carpeta = 'RWF-2000'\n",
        "lista_carpetas_avi = [\n",
        "    {'ruta': f'/content/{carpeta}/train/Fight', 'data': 'train', 'etiqueta':'Fight'},\n",
        "    {'ruta': f'/content/{carpeta}/train/NonFight', 'data': 'train', 'etiqueta':'NonFight'},\n",
        "    {'ruta': f'/content/{carpeta}/val/Fight', 'data': 'val', 'etiqueta':'Fight'},\n",
        "    {'ruta': f'/content/{carpeta}/val/NonFight', 'data': 'val', 'etiqueta':'NonFight'}\n",
        "]\n",
        "lista_archivos_avi = []\n",
        "for carpeta in lista_carpetas_avi:\n",
        "  ruta_carpeta=carpeta['ruta']\n",
        "  tipo_data =carpeta['data']\n",
        "  etiqueta_data =carpeta['etiqueta']\n",
        "  for filename in os.listdir(ruta_carpeta):\n",
        "      if filename.endswith(\".avi\"):\n",
        "        registro_archivo = {'ruta': f'{ruta_carpeta}/{filename}', 'data': f'{tipo_data}', 'etiqueta':f'{etiqueta_data}'}\n",
        "        lista_archivos_avi.append(registro_archivo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm9tPgckXuwJ"
      },
      "outputs": [],
      "source": [
        "videos_RWF2000_df = pd.DataFrame(lista_archivos_avi)\n",
        "videos_RWF2000_df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "xsF0DBc3YI-z",
        "outputId": "931848ee-e2d0-4cbf-b3d3-80a4468aa474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   ruta   data etiqueta\n",
              "count                                               200    200      200\n",
              "unique                                              200      2        2\n",
              "top     /content/RWF-2000/train/Fight/Jq8neqBlL2Y_2.avi  train    Fight\n",
              "freq                                                  1    160      100"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6230b45-cd0f-4643-af10-e48571742ab9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ruta</th>\n",
              "      <th>data</th>\n",
              "      <th>etiqueta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>/content/RWF-2000/train/Fight/Jq8neqBlL2Y_2.avi</td>\n",
              "      <td>train</td>\n",
              "      <td>Fight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6230b45-cd0f-4643-af10-e48571742ab9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a6230b45-cd0f-4643-af10-e48571742ab9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a6230b45-cd0f-4643-af10-e48571742ab9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0b0f4b46-f360-4993-a302-5e2541fc49c2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b0f4b46-f360-4993-a302-5e2541fc49c2')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0b0f4b46-f360-4993-a302-5e2541fc49c2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "\n",
        "# **********************************************************************\n",
        "# Se toma una muestra con una fracción de los datos del dataframe\n",
        "# **********************************************************************\n",
        "\n",
        "# Función para obtener un subconjunto aleatorio del DataFrame\n",
        "def grupo_de_muestras(group, frac=0.1):\n",
        "    return group.sample(frac=frac)\n",
        "\n",
        "# Dividir el DataFrame según las variables 'etapa' y 'etiqueta' y aplicar la función grupo_de_muestras\n",
        "muestra_RWF2000_df = videos_RWF2000_df.groupby(['data', 'etiqueta']).apply(grupo_de_muestras).reset_index(drop=True)\n",
        "# Se elimina una variable que tiene muchas variables\n",
        "del videos_RWF2000_df\n",
        "\n",
        "muestra_RWF2000_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1CcRdj1BGH8"
      },
      "source": [
        "### No es necesario hacer la conversión a .mp4 porque se puede usar directamente el archivo .avi para construir los tensores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEkrXjUKZDh_"
      },
      "outputs": [],
      "source": [
        "# Se crea una función para que convierta un .avi a .mp4 en la misma carpeta con la condición que existan frames\n",
        "\n",
        "def volver_crear_mp4(ruta):\n",
        "  path = ruta\n",
        "  if os.path.exists(ruta):\n",
        "    print(\"Se elimina el archivo: \",path)\n",
        "    os.remove(ruta)\n",
        "    time.sleep(4)\n",
        "  root = os.path.dirname(path)\n",
        "  nombre=path.split('/')[-1:][0].split('.')[0]\n",
        "  ruta_archivo_avi=f'{root}/{nombre}.avi'\n",
        "  ruta_archivo_mp4=f'{root}/{nombre}'\n",
        "\n",
        "  os.popen(\"ffmpeg -i '{input}' -ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4 '{output}.mp4'\".format(input=ruta_archivo_avi, output=ruta_archivo_mp4))\n",
        "  if os.path.exists(f'{ruta_archivo_mp4}.mp4'):\n",
        "    print(\"Se vuelve a crear el archivo: \",path)\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rS_iPflGEJHh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def obtener_tensor(video_paths):\n",
        "  videos = []\n",
        "  for video_path in video_paths:\n",
        "    # Abrir el video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Obtener las características del video\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = num_frames / fps\n",
        "\n",
        "    # Definir la resolución y el número de canales deseados\n",
        "    target_resolution = (224, 224)\n",
        "    target_channels = 3\n",
        "\n",
        "    # Inicializar una lista para almacenar los frames redimensionados\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Número de canales deseado\n",
        "        if frame.shape[-1] != target_channels:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convertir a RGB si es necesario\n",
        "\n",
        "        # Redimensionar el frame a la resolución deseada usando tf.image.resize\n",
        "        frame = tf.image.resize(frame, target_resolution)\n",
        "\n",
        "        # Convertir el frame a tensor y agregarlo a la lista\n",
        "        frame_tensor = tf.convert_to_tensor(frame, dtype=tf.float32) / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    # Apilar los frames en un tensor 4D (frames, alto, ancho, canales)\n",
        "    video_tensor = tf.stack(frames, axis=0)\n",
        "    video_tensor = tf.transpose(video_tensor, [3, 0, 1, 2])\n",
        "    video_tensor = torch.tensor(video_tensor.numpy())\n",
        "    videos.append(video_tensor)\n",
        "    # Liberar los recursos\n",
        "    cap.release()\n",
        "\n",
        "  tensor_torch = torch.stack(videos, dim=0)\n",
        "  # Imprimir las dimensiones del tensor resultante\n",
        "  print(\"Dimensiones del tensor resultante:\", tensor_torch.shape)\n",
        "  return tensor_torch\n",
        "\n",
        "def obtener_labels(video_labels):\n",
        "  # Crear un diccionario de mapeo de etiquetas a valores binarios (1 y 0)\n",
        "  label_map = {'Fight': 1, 'NonFight': 0}\n",
        "\n",
        "  # Convertir las etiquetas a valores binarios usando el mapeo\n",
        "  binary_labels = [label_map[label] for label in video_labels]\n",
        "\n",
        "  # Crear un tensor de PyTorch a partir de la lista de valores binarios\n",
        "  tensor_labels = torch.tensor(binary_labels, dtype=torch.float32).to(torch.long)\n",
        "\n",
        "  # Imprimir las dimensiones del tensor resultante\n",
        "  print(\"Dimensiones del tensor resultante:\", tensor_labels.shape)\n",
        "  return tensor_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_video_paths = list(muestra_RWF2000_df[muestra_RWF2000_df['data']=='train']['ruta'])\n",
        "train_video_labels = list(muestra_RWF2000_df[muestra_RWF2000_df['data']=='train']['etiqueta'])\n",
        "# tensor_data = obtener_tensor()\n",
        "train_videos = obtener_tensor(train_video_paths)\n",
        "train_labels = obtener_labels(train_video_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZozRUEJ4LFw7",
        "outputId": "7f244798-cab2-46e9-c916-ccbbcaac4947"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del tensor resultante: torch.Size([160, 3, 150, 224, 224])\n",
            "Dimensiones del tensor resultante: torch.Size([160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_video_paths = list(muestra_RWF2000_df[muestra_RWF2000_df['data']=='val']['ruta'])\n",
        "test_video_labels = list(muestra_RWF2000_df[muestra_RWF2000_df['data']=='val']['etiqueta'])\n",
        "# tensor_data = obtener_tensor()\n",
        "test_videos = obtener_tensor(test_video_paths)\n",
        "test_labels = obtener_labels(test_video_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0p1gjZCP-rz",
        "outputId": "8d042e40-a766-428d-ee41-de177ff2f802"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del tensor resultante: torch.Size([40, 3, 150, 224, 224])\n",
            "Dimensiones del tensor resultante: torch.Size([40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=TensorDataset(train_videos, train_labels)\n",
        "test_data=TensorDataset(test_videos, test_labels)"
      ],
      "metadata": {
        "id": "ofs16md3PDkk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMGOhH50RMNl"
      },
      "source": [
        "# Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHuJBh9HAR17",
        "outputId": "ba0a185d-3537-48a3-c6a8-dc17d307366d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install einops\n",
        "from torch import nn, einsum\n",
        "import torch\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FSAttention(nn.Module):\n",
        "    \"\"\"Factorized Self-Attention\"\"\"\n",
        "\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class FDAttention(nn.Module):\n",
        "    \"\"\"Factorized Dot-product Attention\"\"\"\n",
        "\n",
        "    def __init__(self, dim, nt, nh, nw, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.nt = nt\n",
        "        self.nh = nh\n",
        "        self.nw = nw\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, d, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "        qs, qt = q.chunk(2, dim=1)\n",
        "        ks, kt = k.chunk(2, dim=1)\n",
        "        vs, vt = v.chunk(2, dim=1)\n",
        "\n",
        "        # Attention over spatial dimension\n",
        "        qs = qs.view(b, h // 2, self.nt, self.nh * self.nw, -1)\n",
        "        ks, vs = ks.view(b, h // 2, self.nt, self.nh * self.nw, -1), vs.view(b, h // 2, self.nt, self.nh * self.nw, -1)\n",
        "        spatial_dots = einsum('b h t i d, b h t j d -> b h t i j', qs, ks) * self.scale\n",
        "        sp_attn = self.attend(spatial_dots)\n",
        "        spatial_out = einsum('b h t i j, b h t j d -> b h t i d', sp_attn, vs)\n",
        "\n",
        "        # Attention over temporal dimension\n",
        "        qt = qt.view(b, h // 2, self.nh * self.nw, self.nt, -1)\n",
        "        kt, vt = kt.view(b, h // 2, self.nh * self.nw, self.nt, -1), vt.view(b, h // 2, self.nh * self.nw, self.nt, -1)\n",
        "        temporal_dots = einsum('b h s i d, b h s j d -> b h s i j', qt, kt) * self.scale\n",
        "        temporal_attn = self.attend(temporal_dots)\n",
        "        temporal_out = einsum('b h s i j, b h s j d -> b h s i d', temporal_attn, vt)\n",
        "\n",
        "        # return self.to_out(out)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class FSATransformerEncoder(nn.Module):\n",
        "    \"\"\"Factorized Self-Attention Transformer Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, nt, nh, nw, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.nt = nt\n",
        "        self.nh = nh\n",
        "        self.nw = nw\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList(\n",
        "                [PreNorm(dim, FSAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                 PreNorm(dim, FSAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                 PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "                 ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b = x.shape[0]\n",
        "        x = torch.flatten(x, start_dim=0, end_dim=1)  # extract spatial tokens from x\n",
        "\n",
        "        for sp_attn, temp_attn, ff in self.layers:\n",
        "            sp_attn_x = sp_attn(x) + x  # Spatial attention\n",
        "\n",
        "            # Reshape tensors for temporal attention\n",
        "            sp_attn_x = sp_attn_x.chunk(b, dim=0)\n",
        "            sp_attn_x = [temp[None] for temp in sp_attn_x]\n",
        "            sp_attn_x = torch.cat(sp_attn_x, dim=0).transpose(1, 2)\n",
        "            sp_attn_x = torch.flatten(sp_attn_x, start_dim=0, end_dim=1)\n",
        "\n",
        "            temp_attn_x = temp_attn(sp_attn_x) + sp_attn_x  # Temporal attention\n",
        "\n",
        "            x = ff(temp_attn_x) + temp_attn_x  # MLP\n",
        "\n",
        "            # Again reshape tensor for spatial attention\n",
        "            x = x.chunk(b, dim=0)\n",
        "            x = [temp[None] for temp in x]\n",
        "            x = torch.cat(x, dim=0).transpose(1, 2)\n",
        "            x = torch.flatten(x, start_dim=0, end_dim=1)\n",
        "\n",
        "        # Reshape vector to [b, nt*nh*nw, dim]\n",
        "        x = x.chunk(b, dim=0)\n",
        "        x = [temp[None] for temp in x]\n",
        "        x = torch.cat(x, dim=0)\n",
        "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FDATransformerEncoder(nn.Module):\n",
        "    \"\"\"Factorized Dot-product Attention Transformer Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, nt, nh, nw, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.nt = nt\n",
        "        self.nh = nh\n",
        "        self.nw = nw\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                PreNorm(dim, FDAttention(dim, nt, nh, nw, heads=heads, dim_head=dim_head, dropout=dropout)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn in self.layers:\n",
        "            x = attn(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViViTBackbone(nn.Module):\n",
        "    \"\"\" Model-3 backbone of ViViT \"\"\"\n",
        "\n",
        "    def __init__(self, t, h, w, patch_t, patch_h, patch_w, num_classes, dim, depth, heads, mlp_dim, dim_head=3,\n",
        "                 channels=3, mode='tubelet', device='cuda', emb_dropout=0., dropout=0., model=3):\n",
        "        super().__init__()\n",
        "\n",
        "        assert t % patch_t == 0 and h % patch_h == 0 and w % patch_w == 0, \"Video dimensions should be divisible by \" \\\n",
        "                                                                           \"tubelet size \"\n",
        "\n",
        "        self.T = t\n",
        "        self.H = h\n",
        "        self.W = w\n",
        "        self.channels = channels\n",
        "        self.t = patch_t\n",
        "        self.h = patch_h\n",
        "        self.w = patch_w\n",
        "        self.mode = mode\n",
        "        self.device = device\n",
        "\n",
        "        self.nt = self.T // self.t\n",
        "        self.nh = self.H // self.h\n",
        "        self.nw = self.W // self.w\n",
        "\n",
        "        tubelet_dim = self.t * self.h * self.w * channels\n",
        "\n",
        "        self.to_tubelet_embedding = nn.Sequential(\n",
        "            Rearrange('b c (t pt) (h ph) (w pw) -> b t (h w) (pt ph pw c)', pt=self.t, ph=self.h, pw=self.w),\n",
        "            nn.Linear(tubelet_dim, dim)\n",
        "        )\n",
        "\n",
        "        # repeat same spatial position encoding temporally\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, self.nh * self.nw, dim)).repeat(1, self.nt, 1, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        if model == 3:\n",
        "            self.transformer = FSATransformerEncoder(dim, depth, heads, dim_head, mlp_dim,\n",
        "                                                     self.nt, self.nh, self.nw, dropout)\n",
        "        elif model == 4:\n",
        "            assert heads % 2 == 0, \"Number of heads should be even\"\n",
        "            self.transformer = FDATransformerEncoder(dim, depth, heads, dim_head, mlp_dim,\n",
        "                                                     self.nt, self.nh, self.nw, dropout)\n",
        "\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" x is a video: (b, C, T, H, W) \"\"\"\n",
        "\n",
        "        tokens = self.to_tubelet_embedding(x)\n",
        "\n",
        "        tokens += self.pos_embedding.to(device)\n",
        "        tokens = self.dropout(tokens)\n",
        "\n",
        "        x = self.transformer(tokens)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     device = torch.device('cuda')\n",
        "#     x = torch.rand(32, 3, 32, 64, 64).to(device)\n",
        "\n",
        "#     vivit = ViViTBackbone(32, 64, 64, 8, 4, 4, 10, 512, 6, 10, 8, model=3).to(device)\n",
        "#     out = vivit(x)\n",
        "#     print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxP1YUfKKIpB",
        "outputId": "e32e2966-db6f-4090-aa38-270cd24a8f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.1343\n",
            "Epoch [2/100], Loss: 0.7603\n",
            "Epoch [3/100], Loss: 0.9734\n",
            "Epoch [4/100], Loss: 0.6929\n",
            "Epoch [5/100], Loss: 0.8139\n",
            "Epoch [6/100], Loss: 0.6735\n",
            "Epoch [7/100], Loss: 0.6921\n",
            "Epoch [8/100], Loss: 0.7046\n",
            "Epoch [9/100], Loss: 0.6851\n",
            "Epoch [10/100], Loss: 0.6920\n",
            "Epoch [11/100], Loss: 0.6764\n",
            "Epoch [12/100], Loss: 0.7330\n",
            "Epoch [13/100], Loss: 0.6714\n",
            "Epoch [14/100], Loss: 0.6705\n",
            "Epoch [15/100], Loss: 0.6819\n",
            "Epoch [16/100], Loss: 0.6708\n",
            "Epoch [17/100], Loss: 0.6800\n",
            "Epoch [18/100], Loss: 0.6588\n",
            "Epoch [19/100], Loss: 0.6382\n",
            "Epoch [20/100], Loss: 0.6694\n",
            "Epoch [21/100], Loss: 0.6319\n",
            "Epoch [22/100], Loss: 0.6029\n",
            "Epoch [23/100], Loss: 0.6457\n",
            "Epoch [24/100], Loss: 0.6664\n",
            "Epoch [25/100], Loss: 0.6109\n",
            "Epoch [26/100], Loss: 0.6202\n",
            "Epoch [27/100], Loss: 0.6375\n",
            "Epoch [28/100], Loss: 0.6040\n",
            "Epoch [29/100], Loss: 0.6556\n",
            "Epoch [30/100], Loss: 0.6247\n",
            "Epoch [31/100], Loss: 0.6071\n",
            "Epoch [32/100], Loss: 0.5607\n",
            "Epoch [33/100], Loss: 0.6053\n",
            "Epoch [34/100], Loss: 0.5787\n",
            "Epoch [35/100], Loss: 0.6229\n",
            "Epoch [36/100], Loss: 0.6405\n",
            "Epoch [37/100], Loss: 0.6683\n",
            "Epoch [38/100], Loss: 0.5224\n",
            "Epoch [39/100], Loss: 0.5811\n",
            "Epoch [40/100], Loss: 0.6301\n",
            "Epoch [41/100], Loss: 0.5864\n",
            "Epoch [42/100], Loss: 0.5730\n",
            "Epoch [43/100], Loss: 0.5636\n",
            "Epoch [44/100], Loss: 0.5362\n",
            "Epoch [45/100], Loss: 0.6028\n",
            "Epoch [46/100], Loss: 0.5294\n",
            "Epoch [47/100], Loss: 0.6058\n",
            "Epoch [48/100], Loss: 0.5179\n",
            "Epoch [49/100], Loss: 0.6214\n",
            "Epoch [50/100], Loss: 0.5408\n",
            "Epoch [51/100], Loss: 0.5658\n",
            "Epoch [52/100], Loss: 0.5313\n",
            "Epoch [53/100], Loss: 0.5601\n",
            "Epoch [54/100], Loss: 0.5392\n",
            "Epoch [55/100], Loss: 0.4926\n",
            "Epoch [56/100], Loss: 0.6070\n",
            "Epoch [57/100], Loss: 0.5984\n",
            "Epoch [58/100], Loss: 0.5375\n",
            "Epoch [59/100], Loss: 0.6813\n",
            "Epoch [60/100], Loss: 0.5648\n",
            "Epoch [61/100], Loss: 0.5019\n",
            "Epoch [62/100], Loss: 0.6493\n",
            "Epoch [63/100], Loss: 0.5158\n",
            "Epoch [64/100], Loss: 0.4858\n",
            "Epoch [65/100], Loss: 0.4354\n",
            "Epoch [66/100], Loss: 0.7138\n",
            "Epoch [67/100], Loss: 0.5467\n",
            "Epoch [68/100], Loss: 0.6077\n",
            "Epoch [69/100], Loss: 0.7011\n",
            "Epoch [70/100], Loss: 0.4935\n",
            "Epoch [71/100], Loss: 0.6355\n",
            "Epoch [72/100], Loss: 0.5303\n",
            "Epoch [73/100], Loss: 0.5296\n",
            "Epoch [74/100], Loss: 0.6955\n",
            "Epoch [75/100], Loss: 0.5366\n",
            "Epoch [76/100], Loss: 0.4119\n",
            "Epoch [77/100], Loss: 0.4939\n",
            "Epoch [78/100], Loss: 0.5128\n",
            "Epoch [79/100], Loss: 0.5324\n",
            "Epoch [80/100], Loss: 0.5824\n",
            "Epoch [81/100], Loss: 0.5107\n",
            "Epoch [82/100], Loss: 0.4959\n",
            "Epoch [83/100], Loss: 0.5636\n",
            "Epoch [84/100], Loss: 0.5793\n",
            "Epoch [85/100], Loss: 0.4658\n",
            "Epoch [86/100], Loss: 0.5331\n",
            "Epoch [87/100], Loss: 0.4871\n",
            "Epoch [88/100], Loss: 0.5544\n",
            "Epoch [89/100], Loss: 0.4663\n",
            "Epoch [90/100], Loss: 0.5378\n",
            "Epoch [91/100], Loss: 0.5179\n",
            "Epoch [92/100], Loss: 0.4368\n",
            "Epoch [93/100], Loss: 0.6156\n",
            "Epoch [94/100], Loss: 0.5263\n",
            "Epoch [95/100], Loss: 0.4794\n",
            "Epoch [96/100], Loss: 0.4812\n",
            "Epoch [97/100], Loss: 0.5187\n",
            "Epoch [98/100], Loss: 0.4771\n",
            "Epoch [99/100], Loss: 0.5058\n",
            "Epoch [100/100], Loss: 0.4916\n",
            "Accuracy of the model on the test videos: 50.0%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Configuración del modelo y del dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "v = ViViTBackbone(\n",
        "    t=150,\n",
        "    h=224,\n",
        "    w=224,\n",
        "    patch_t=30,\n",
        "    patch_h=28,\n",
        "    patch_w=28,\n",
        "    num_classes=2,  # Solo hay 2 clases: violencia y no violencia\n",
        "    dim=224,\n",
        "    depth=6,\n",
        "    heads=10,\n",
        "    mlp_dim=5,\n",
        "    model=3\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "# train_data=TensorDataset(train_videos, train_labels)\n",
        "# test_data=TensorDataset(test_videos, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "# Función de pérdida y optimizador\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(v.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "\n",
        "# Entrenamiento\n",
        "loss_values = []\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    avg_loss = 0\n",
        "    num_batches = 0\n",
        "    for videos_batch, labels_batch in train_loader:\n",
        "        videos_batch, labels_batch = videos_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        # Propagación hacia adelante\n",
        "        predictions = v(videos_batch)\n",
        "        loss = criterion(predictions, labels_batch)\n",
        "\n",
        "        # Propagación hacia atrás y optimización\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    avg_loss /= num_batches\n",
        "    loss_values.append(avg_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluación\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for videos_batch, labels_batch in test_loader:\n",
        "        videos_batch, labels_batch = videos_batch.to(device), labels_batch.to(device)\n",
        "        predictions = v(videos_batch)\n",
        "        _, predicted = torch.max(predictions.data, 1)\n",
        "        total += labels_batch.size(0)\n",
        "        correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the model on the test videos: {100 * correct / total}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "C7nfS3ZAUZe4",
        "outputId": "fde13a64-c673-4246-8b38-08a1267b0d05"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1WbGaqbQheq",
        "outputId": "20459ada-d9c3-45a6-f4c4-ebc3fb3cfccc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([120, 3, 32, 64, 64])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "videos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIKYLiRfNUYE",
        "outputId": "cb0a12d5-5fe7-49a0-c4d4-6b3a065d3d92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'NonFight',\n",
              " 'Fight',\n",
              " 'Fight',\n",
              " 'NonFight',\n",
              " 'NonFight']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "857CPsM__3V-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ReAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.reattn_weights = nn.Parameter(torch.randn(heads, heads))\n",
        "\n",
        "        self.reattn_norm = nn.Sequential(\n",
        "            Rearrange('b h i j -> b i j h'),\n",
        "            nn.LayerNorm(heads),\n",
        "            Rearrange('b i j h -> b h i j')\n",
        "        )\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        # attention\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        # re-attention\n",
        "\n",
        "        attn = einsum('b h i j, h g -> b g i j', attn, self.reattn_weights)\n",
        "        attn = self.reattn_norm(attn)\n",
        "\n",
        "        # aggregate and out\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class LeFF(nn.Module):\n",
        "\n",
        "    def __init__(self, dim = 192, scale = 4, depth_kernel = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        scale_dim = dim*scale\n",
        "        self.up_proj = nn.Sequential(nn.Linear(dim, scale_dim),\n",
        "                                    Rearrange('b n c -> b c n'),\n",
        "                                    nn.BatchNorm1d(scale_dim),\n",
        "                                    nn.GELU(),\n",
        "                                    Rearrange('b c (h w) -> b c h w', h=14, w=14)\n",
        "                                    )\n",
        "\n",
        "        self.depth_conv =  nn.Sequential(nn.Conv2d(scale_dim, scale_dim, kernel_size=depth_kernel, padding=1, groups=scale_dim, bias=False),\n",
        "                          nn.BatchNorm2d(scale_dim),\n",
        "                          nn.GELU(),\n",
        "                          Rearrange('b c h w -> b (h w) c', h=14, w=14)\n",
        "                          )\n",
        "\n",
        "        self.down_proj = nn.Sequential(nn.Linear(scale_dim, dim),\n",
        "                                    Rearrange('b n c -> b c n'),\n",
        "                                    nn.BatchNorm1d(dim),\n",
        "                                    nn.GELU(),\n",
        "                                    Rearrange('b c n -> b n c')\n",
        "                                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LCAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "        q = q[:, :, -1, :].unsqueeze(2) # Only Lth element use as query\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GycmS4Mo5KA-",
        "outputId": "b8271ef8-9cbd-4c20-bd2a-f3f3e70bda4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable Parameters: 4.328M\n",
            "Shape of out : torch.Size([1, 100])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "class ViViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, num_frames, dim = 192, depth = 4, heads = 3, pool = 'cls', in_channels = 3, dim_head = 64, dropout = 0.,\n",
        "                 emb_dropout = 0., scale_dim = 4, ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = in_channels * patch_size ** 2\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_frames, num_patches + 1, dim))\n",
        "        self.space_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.space_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)\n",
        "\n",
        "        self.temporal_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.temporal_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.pool = pool\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.to_patch_embedding(x)\n",
        "        b, t, n, _ = x.shape\n",
        "\n",
        "        cls_space_tokens = repeat(self.space_token, '() n d -> b t n d', b = b, t=t)\n",
        "        x = torch.cat((cls_space_tokens, x), dim=2)\n",
        "        x += self.pos_embedding[:, :, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = rearrange(x, 'b t n d -> (b t) n d')\n",
        "        x = self.space_transformer(x)\n",
        "        x = rearrange(x[:, 0], '(b t) ... -> b t ...', b=b)\n",
        "\n",
        "        cls_temporal_tokens = repeat(self.temporal_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_temporal_tokens, x), dim=1)\n",
        "\n",
        "        x = self.temporal_transformer(x)\n",
        "\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    img = torch.ones([1, 16, 3, 224, 224]).cuda()\n",
        "\n",
        "    model = ViViT(224, 16, 100, 16).cuda()\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "    print('Trainable Parameters: %.3fM' % parameters)\n",
        "\n",
        "    out = model(img)\n",
        "\n",
        "    print(\"Shape of out :\", out.shape)      # [B, num_classes]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLRhb4IwsSOVhbo3cDhjE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}